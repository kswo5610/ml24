{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually updating the Q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T15:00:12.473386Z",
     "start_time": "2019-06-06T15:00:12.469511Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T15:00:12.791737Z",
     "start_time": "2019-06-06T15:00:12.728921Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")  # a very simple application\n",
    "done = False\n",
    "\n",
    "# Resets the state of the environment and returns an initial observation.\n",
    "env.reset()  # 2 observables - position and velocity\n",
    "\n",
    "# number of allowed actions  - 3: left push, no movement, right push\n",
    "# print(env.action_space.n)\n",
    "\n",
    "# there are only two observables - position and velocity\n",
    "# print(env.observation_space.high)  # the high values of the observations\n",
    "# print(env.observation_space.low)  # the low values\n",
    "\n",
    "# the range of values for observation 1 is 0.6 to -1.2\n",
    "# and similarly for observation 2 its 0.07 to -0.07\n",
    "# we can segregate the values in 20 chunks (can be any value)\n",
    "\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)  # will give out 20*20 list\n",
    "\n",
    "# see how big is the range for each of the 20 different buckets\n",
    "discrete_os_win_size = (env.observation_space.high -\n",
    "                        env.observation_space.low) / DISCRETE_OS_SIZE\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "DISCOUNT = 0.95  # how important we find the new future actions are ; future reward over current reward\n",
    "EPISODES = 2000\n",
    "render = True\n",
    "\n",
    "# even though the solution might have been found, we still wish to look for other solutions\n",
    "epsilon = 0.5  # 0-1 ; higher it is, more likely for it to perform something random action\n",
    "START_EPSILON_DECAYING = 1\n",
    "# python2 style division - gives only int values\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "# Q learning\n",
    "# so we will have now a table such that each row will have 400 (20*20) rows for the possible state the agent can be in\n",
    "# and 3 columns for the 3 possible actions\n",
    "# the agent will see which state it is in and take the action corresponding to the largest Q value\n",
    "\n",
    "# Create a randomised q_table and agent will update it after exploring the environment\n",
    "q_table = np.random.uniform(\n",
    "    low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    \"\"\"Discretizes continuous state into discrete bins.\"\"\"\n",
    "    \n",
    "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
    "    discrete_state = tuple(discrete_state.astype(int))  # use int instead of np.int\n",
    "    \n",
    "    assert all(0 <= s < DISCRETE_OS_SIZE[i] for i, s in enumerate(discrete_state)), f\"State {discrete_state} out of bounds\"\n",
    "    return discrete_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kswo/miniconda3/envs/homl3/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.goal_position to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.goal_position` for environment variables or `env.get_wrapper_attr('goal_position')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# modified version\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    discrete_state = get_discrete_state(state)\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[discrete_state])  # Exploit\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)  # Explore\n",
    "\n",
    "        # Sanity check for action\n",
    "        assert 0 <= action < env.action_space.n, f\"Action {action} out of bounds\"\n",
    "\n",
    "        # Step through the environment\n",
    "        #print(env.step(action) ) \n",
    "        \n",
    "        new_state, reward, done, info , _  = env.step(action)\n",
    "        new_state = new_state\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        # Sanity check for discrete states and new discrete state\n",
    "        assert all(0 <= s < DISCRETE_OS_SIZE[i] for i, s in enumerate(new_discrete_state)), f\"New state {new_discrete_state} out of bounds\"\n",
    "\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if END_EPSILON_DECAYING >= ep >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T08:33:14.619136Z",
     "start_time": "2019-06-02T08:33:14.609466Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
